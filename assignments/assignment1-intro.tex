\documentclass[a4paper]{article}
\usepackage[notheorems]{beamerarticle}

\input{../preamble}

\def\solution {0}


\title{IN-STK 5000: Introductory assignment} 
\author{Christos Dimitrakakis}
\begin{document}
\maketitle
The purpose of this assignment is to evaluate the background knowledge
of the students in the course. Please provide as precise and concise
answers as possible. 

\section{Probability theory}
In this section we consider probability as a measure, i.e. as a function from sets to $[0,1]$. All events are subsets of the universal set $\Omega$, so that $P(\Omega) = 1$, $P(\emptyset) = 0$.
\begin{exercise}
  If $A, B$ are mutually exclusive events i.e. $A \cap B = \emptyset$,  then 
  \[
  P(A \cup B) =
  \ifdefined \solution
  P(A) + P(B)
  \else
  \ldots
  \fi
  \]
\end{exercise}
\begin{exercise}[Union bound]
  If $A, B$ are not exclusive events, i.e. $A \cap B \neq \emptyset$, then 
  \[
  P(A \cup B) \leq
  \ifdefined \solution
  P(A) + P(B)
  \else
  \ldots
  \fi
  \]
\end{exercise}

\begin{exercise}[Conditional probability]
  If $A, B$ are two events, with $P(B) > 0$, then conditional probability is defined as
  \[
  P(A \mid B) \defn 
  \ifdefined \solution
  \frac{P(A \cap B)}{P(B)}
  \else
  \ldots
  \fi
  \]
\end{exercise}

\begin{exercise}[Marginal probability]
  Let $A_1, \ldots, A_n$ be mutually exclusive events so that $\bigcup_{i=1}^n A_i = \Omega$ and $B \subset \Omega$ an arbitrary other event. Then:
  \[
  P(B) = \sum_{A_i} 
  \ifdefined \solution
  P(A_i \cap B)
  =
  \sum_{A_i} 
  P(B \mid A_i)
  P(A_i )
  \else
  \ldots
  \fi
  \]
\end{exercise}

\section{Random variables and statistics}


\begin{exercise}
  A real-valued random variable $x$ is simply a mapping $x : \Omega \to \Reals$.
  Write the definition of the expectation of $x$ drawn from $P$, where $P$ is a probability measure on $(\Omega, \Sigma)$ and $\Sigma$ is the $\sigma$-algebra generated by $\Omega$.
  \[
  \E(x) = 
  \ifdefined \solution
  \sum_{\omega \in \Omega} x(\omega) P(\omega) 
  \else
  \ldots
  \fi
  \]
\end{exercise}

\begin{exercise}
  The sample mean $\mu_n$ of $n$  i.i.d random variables $x_1, \ldots, x_n$ is defined as
  \[
  \mu_n \defn
  \ifdefined \solution
  \frac{1}{n} \sum_{i=1}^n x_i
  \else
  \ldots 
  \fi
  \]
\end{exercise}

\begin{exercise}
  Write the expectation of the sample mean $\mu_n$ in relation to $x_1, \ldots, x_n$.
  \ifdefined \solution
  Since $x_i$ are i.i.d, there is some $\bar{x}$ so that $\E x_i = \bar{x}$ for all $i$. Then
  \fi
  \[
  \E \mu_n =
  \ifdefined \solution
  \E \frac{1}{n} \sum_{i=1}^n x_i
  = \frac{1}{n} \sum_{i=1}^n \E x_i = \bar{x}
  \else
  \ldots
  \fi
  \]

\end{exercise}

\begin{exercise}
  A null hypothesis test at significance level $p$ is constructed by using a test statistic $\pol: \CX \to [0,1)$ mapping from the space of possible data to the interval $[0,1)$, so that the test rejects the null hypothesis whenever $\pol(x) < p$. Does this mean that:
  \begin{enumerate}
  \item The probability that the test will falsely reject the null hypothesis is $p$.
  \item The probability that the test will falsely accept the null hypothesis is $p$.
  \item The probability that the test will falsely reject the alternative hypothesis is $p$.
  \item The probability that the test will falsely accept the alternative hypothesis is $p$.
  \item Given the data $x$, the probability that the null hypothesis is true is $\pol(x)$.
  \item Given the data $x$, the probability that the null hypothesis is false is $\pol(x)$.
  \item Given the data $x$, the probability that the alternative hypothesis is true is $\pol(x)$.
  \item Given the data $x$, the probability that the alternative hypothesis is false is $\pol(x)$.
  \end{enumerate}
  \end{exercise}
  \ifdefined \solution
  Null hypothesis tests that have a fixed significance level $p$ are designed so that, if the data comes from a given null hypothesis, then the probability that the test statistic $\pol(x) < p$ is exactly equal to $p$. The probability of falsely accepting the null hypothesis, however, depends on the unknown alternative hypothesis and so cannot be computed. Consequently the correct answers are $1$ and $4$ (Since the decision rule either accepts or rejects the null hypothesis, $4$ is correct too).
  \fi
\section{Linear algebra}

\begin{exercise}
  If $\bx = x_1, \ldots, x_n$, $\by = y_1, \ldots, y_n$ are two column vectors in $\Reals^n$, what is their inner product:
  \[
  \bx \cdot \by = \bx^\top \by = 
  \ifdefined \solution
  \sum_{i=1}^n x_i y_i
  \fi
  \]
\end{exercise}

\begin{exercise}
  The matrix 
  \[
  \MA^+ \defn (\MA^\top \MA)^{-1} \MA^\top.
  \]
  is the left-pseudoinverse of $\MA$. Complete the following:
  \[
  \MA^+ \MA =
  \ifdefined \solution
  (\MA^\top \MA)^{-1} \MA^\top \MA
  = 
  \eye
  \fi
  \]
\end{exercise}


\section{Calculus}

\begin{exercise}
  If $f : \CX \to \Reals$ is a twice-differentiable function, what are \emph{sufficient} conditions for $x_0$ to be a \emph{local maximum} of the function, i.e. there exists $\epsilon > 0$ so that $f(x_0) \geq f(x)$ for all $x : |x - x_0| < \epsilon$?
  \ifdefined\solution
  If $d f(x_0) /dx  = 0$ then
  $x_0$ is either a saddle point, a maximum or a minimum. If in addition $d^2 f(x_0) /dx^2 < 0$, then $x_0$ is a maximum.
  \fi
\end{exercise}

\begin{exercise}
  Solve the following integral, for $T > 0$
  \[
  \int_{1}^T \frac{1}{x} \dd x =
  \ifdefined\solution
  \ln T
  \else 
  \ldots
  \fi
  \]
\end{exercise}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
