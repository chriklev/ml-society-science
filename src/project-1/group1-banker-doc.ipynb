{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model development\n",
    "\n",
    "\n",
    "## Policy\n",
    "\n",
    "We assume that $A = \\{a_{0}, a_{1}\\}$, we further define $a_{0}$ as \"deny credit\" and $a_{1}$ as \"offer credit\". We also know from the project text that the utility function is assumed to be linear. This implies that among $E(utility|a_{i})$ and $E(utility|a_{j})$, we will always choose action $a_{i}$ over action $a_{j}$ if $E(utility|a_{i}) > E(utility|a_{j})$ adapted from (Dimitrakakis, 2020, p. 48). We will \"blindly\" choose the action that maximizes expected utility.\n",
    "\n",
    "## Utility function\n",
    "\n",
    "There are two possible actions that we could perform: $\\{a_{0}, a_{1}\\}$. We also have the following rewards, which are defined as possible outcomes for the bank (Dimitrakakis, 2020, p. 47). In our case these rewards are $\\{-m, 0, m((1 + r)^{n} - 1)\\}$. Further there are two possible actions for the bank when it comes to deciding for each new customer if they will be granted credit.\n",
    "\n",
    "Further, we will use the definition of expected utility\n",
    "$$\n",
    "E[U|a_{t} = a] = \\sum_{r} U(r)Pr(r|a_{t} = a)\n",
    "$$\n",
    "adapted from (Dimitrakakis, 2020, p. 48). This will be used for each action to calculate its expected utility. We can further define the rewards as $r_{0} =$ \"the debtor defaults\" and $r_{1} =$ \"the debtor does not default\". If we use the assumption that the utility function is linear, we can say that $U(r)$ is proportional to $r$.\n",
    "\n",
    "### Grant credit\n",
    "\n",
    "If we decide to grant credit ($a_{t} = 1 $) we have the following expected utility considering the rewards above:\n",
    "$$\n",
    "E[U|a_{t} = 1] = U(r = r_{0})Pr(r = r_{0}|a_{t} = 1) + U(r = r_{1})Pr(r = r_{1}|a_{t} = 1) \n",
    "$$\n",
    "which becomes\n",
    "$$\n",
    "E[U|a_{t} = 1] = -m \\cdot Pr(r = r_{0}|a_{t} = 1) + m((1 + r)^{n} - 1) \\cdot Pr(r = r_{1}|a_{t} = 1)\n",
    "$$\n",
    "In our code, we have defined $Pr(r = r_{1}|a_{t} = 1)$ as the variable \"p\\_c\" and $Pr(r = r_{0}|a_{t} = 1)$ as 1-\"p\\_c\".\n",
    "\n",
    "### Do not grant credit\n",
    "\n",
    "If we decide not to grant credit ($a_{t} = 0 $).\n",
    "\n",
    "$$\n",
    "E[U|a_{t} = 0] = 0 \\cdot Pr(r = r_{0}|a_{t} = 0) + 0 \\cdot Pr(r = r_{1}|a_{t} = 0) = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def expected_utility(self, X, action):\n",
    "    \"\"\"Calculate expected utility using the decision maker model.\n",
    "\n",
    "    Args:\n",
    "        X: New observations.\n",
    "        action: Whether or not to grant the loan.\n",
    "\n",
    "    Returns:\n",
    "        The expected utilities of the decision maker.\n",
    "    \"\"\"\n",
    "    if action == 0:\n",
    "        return np.zeros(X.shape[0])\n",
    "\n",
    "    r = self.rate\n",
    "    p_c = self.predict_proba(X)\n",
    "\n",
    "    # duration in months\n",
    "    n = X['duration']\n",
    "    # amount\n",
    "    m = X['amount']\n",
    "\n",
    "    e_x = p_c * m * ((1 + r) ** n - 1) + (1 - p_c) * (-m)\n",
    "    return e_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a model\n",
    "\n",
    "We chose to use a logistic regression model. It predicts the probability of a binary categorical variable beeing 1. A fresh random state is also given to the model for reproducable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fit_model(self, X, y):\n",
    "    \"\"\"Fits the logistic model.\n",
    "\n",
    "    Args:\n",
    "        X: Covariates\n",
    "        y: Response variable\n",
    "\n",
    "    Notes:\n",
    "        Using logistic regression, adapted from\n",
    "        https://scikit-learn.org/stable/modules/generated/\n",
    "            sklearn.linear_model.LogisticRegression.html\n",
    "    \"\"\"\n",
    "    log_reg_object = LogisticRegression(random_state=1, max_iter=2000)\n",
    "    return log_reg_object.fit(X, y)\n",
    "\n",
    "def predict_proba(self, X):\n",
    "    \"\"\"Predicts the probability for y=1 given new observations.\n",
    "\n",
    "    Args:\n",
    "        x: New, independent observations.\n",
    "\n",
    "    Returns:\n",
    "        The predicted probabilities for y=1.\n",
    "    \"\"\"\n",
    "    return self.model.predict_proba(X)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When reading the data we one hot encode all the catagorical variables which means that they loose the information in the order. This could be fixed by instead giving them an integer value, but then we assume a linear relationship between the order of the categories.\n",
    "\n",
    "## Best action\n",
    "\n",
    "The best action is the action that gives the highest utility. In the event of the utilities beeing equal, we chose to not give a loan. Because of the linear utility of the investor it does not matter what we do in this situation, but we figured it is better to not accept unnecessary variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_action(self, X):\n",
    "    \"\"\"Gets the best actions defined as the actions that maximizes utility.\n",
    "    An epsilon for utility is also set as the threshold that the expected\n",
    "    utility should exceed in order to get the best action. This utility\n",
    "    epsilon is 0 if the banker is not configured to use this functionality.\n",
    "    Otherwise it is estimated from the training data as the value that\n",
    "    provide a type 1 error below the parameter '_max_type1_error'.\n",
    "\n",
    "    Args:\n",
    "        X: New observations.\n",
    "\n",
    "    Returns:\n",
    "        Best actions based on maximizing utility.\n",
    "    \"\"\"\n",
    "    expected_utility_give_loan = self.expected_utility(X, 1)\n",
    "    expected_utility_no_loan = self.expected_utility(X, 0)\n",
    "\n",
    "    give_loan = expected_utility_give_loan > (\n",
    "        expected_utility_no_loan + self._utility_epsilon)\n",
    "    return give_loan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model against random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 21 columns):\n #   Column                    Non-Null Count  Dtype \n---  ------                    --------------  ----- \n 0   checking account balance  1000 non-null   object\n 1   duration                  1000 non-null   int64 \n 2   credit history            1000 non-null   object\n 3   purpose                   1000 non-null   object\n 4   amount                    1000 non-null   int64 \n 5   savings                   1000 non-null   object\n 6   employment                1000 non-null   object\n 7   installment               1000 non-null   int64 \n 8   marital status            1000 non-null   object\n 9   other debtors             1000 non-null   object\n 10  residence time            1000 non-null   int64 \n 11  property                  1000 non-null   object\n 12  age                       1000 non-null   int64 \n 13  other installments        1000 non-null   object\n 14  housing                   1000 non-null   object\n 15  credits                   1000 non-null   int64 \n 16  job                       1000 non-null   object\n 17  persons                   1000 non-null   int64 \n 18  phone                     1000 non-null   object\n 19  foreign                   1000 non-null   object\n 20  repaid                    1000 non-null   int64 \ndtypes: int64(8), object(13)\nmemory usage: 164.2+ KB\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import random_banker\n",
    "import group1_banker\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(24092020)\n",
    "\n",
    "features = ['checking account balance', 'duration', 'credit history',\n",
    "            'purpose', 'amount', 'savings', 'employment', 'installment',\n",
    "            'marital status', 'other debtors', 'residence time',\n",
    "            'property', 'age', 'other installments', 'housing', 'credits',\n",
    "            'job', 'persons', 'phone', 'foreign', 'repaid']\n",
    "\n",
    "data_raw = pd.read_csv(\"german.data\",\n",
    "                 delim_whitespace=True,\n",
    "                 names=features)\n",
    "data_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "numeric_variables = ['duration', 'age', 'residence time', 'installment',\n",
    "             'amount', 'persons', 'credits']\n",
    "data = data_raw[numeric_variables]\n",
    "\n",
    "# Mapping the response to 0 and 1\n",
    "data[\"repaid\"] = data_raw[\"repaid\"].map({1:1, 2:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create dummy variables for all the catagorical variables\n",
    "not_dummy_names = numeric_variables + [\"repaid\"]\n",
    "dummy_names = [x not in not_dummy_names for x in features]\n",
    "dummies = pd.get_dummies(data_raw.iloc[:,dummy_names], drop_first=True)\n",
    "data = data.join(dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing decision makers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_data():\n",
    "    \"\"\" Reads in raw data and only maps response to 0 and 1\n",
    "    \"\"\"\n",
    "    features = ['checking account balance', 'duration', 'credit history',\n",
    "                'purpose', 'amount', 'savings', 'employment', 'installment',\n",
    "                'marital status', 'other debtors', 'residence time',\n",
    "                'property', 'age', 'other installments', 'housing', 'credits',\n",
    "                'job', 'persons', 'phone', 'foreign', 'repaid']\n",
    "\n",
    "    data_raw = pd.read_csv(\"../../data/credit/german.data\",\n",
    "                           delim_whitespace=True, names=features)\n",
    "\n",
    "    # Mapping the response to 0 and 1\n",
    "    data_raw.loc[:, \"repaid\"] = data_raw[\"repaid\"].map({1: 1, 2: 0})\n",
    "\n",
    "    categorical_columns = ['checking account balance', 'credit history',\n",
    "                           'purpose', 'savings', 'employment', 'marital status',\n",
    "                           'other debtors', 'property', 'other installments',\n",
    "                           'housing', 'job', 'phone', 'foreign', 'repaid']\n",
    "    data_raw.loc[:, categorical_columns] = data_raw[categorical_columns].apply(\n",
    "        lambda x: x.astype('category'))\n",
    "\n",
    "    return data_raw\n",
    "\n",
    "\n",
    "def one_hot_encode(data):\n",
    "    \"\"\" One hot encodes specified columns.\n",
    "    \"\"\"\n",
    "    columns = ['checking account balance', 'credit history',\n",
    "               'purpose', 'savings', 'employment', 'marital status',\n",
    "               'other debtors', 'property', 'other installments',\n",
    "               'housing', 'job', 'phone', 'foreign']\n",
    "    dummies = pd.get_dummies(data[columns], drop_first=True)\n",
    "    data = data.drop(columns, axis=1)\n",
    "\n",
    "    return data.join(dummies)\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    data = get_raw_data()\n",
    "    data = one_hot_encode(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utility_from_obs(predicted_decision, true_decision, amount, duration, interest_rate):\n",
    "    \"\"\"Calculates utility from a single observation.\n",
    "\n",
    "    Args:\n",
    "        predicted_decision: the model's best actions\n",
    "        true_decision: if the observations repaid or not\n",
    "        amount: the lending amount\n",
    "        duration: the number of periods\n",
    "        interest_rate: the interest rate of the loan\n",
    "\n",
    "    Returns:\n",
    "        The utility from the single observation given our action.\n",
    "    \"\"\"\n",
    "    utility = np.zeros_like(true_decision)\n",
    "\n",
    "    predicted_decision_bool = predicted_decision == 1\n",
    "    ind1 = np.logical_and(predicted_decision_bool, true_decision == 1)\n",
    "    ind2 = np.logical_and(predicted_decision_bool, true_decision == 0)\n",
    "\n",
    "    utility[ind1] = amount[ind1]*((1 + interest_rate)**duration[ind1] - 1)\n",
    "    utility[ind2] = -amount[ind2]\n",
    "\n",
    "    return utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utility_from_test_set(X, y, decision_maker, interest_rate):\n",
    "    \"\"\"Calculates total utility from a given test set.\n",
    "\n",
    "    Args:\n",
    "        X: the covariates of the test set\n",
    "        y: the response variable of the test set\n",
    "        decision_maker: the decision maker to use in order to calculate utility\n",
    "        interest_rate: the interest rate to use when calculating utility\n",
    "\n",
    "    Returns:\n",
    "        The sum of utility from the test set and the sum of utility divided by\n",
    "        total amount.\n",
    "    \"\"\"\n",
    "    predicted_decision = decision_maker.get_best_action(X)\n",
    "\n",
    "    amount = X['amount']\n",
    "    duration = X['duration']\n",
    "\n",
    "    utility = utility_from_obs(\n",
    "        predicted_decision, y, amount, duration, interest_rate)\n",
    "\n",
    "    return np.sum(utility), np.sum(utility)/np.sum(amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeated_cross_validation_utility(X, y, bankers, interest_rate, n_repeats=20, n_folds=5):\n",
    "    \"\"\" Preforms repeated cross validation to find estimates for average utility\n",
    "    for different bankers.\n",
    "\n",
    "    Args:\n",
    "        X: pandas data frame with covariates\n",
    "        y: pandas series with the response\n",
    "        bankers: iterable with bankers implementing the fit() and get_best_action() methods.\n",
    "        interest_rate: float interest rate by month\n",
    "        n_repeats: number of repeats in repeated cross validation\n",
    "        n_folds: number of folds in k-fold cross validation\n",
    "\n",
    "    Returns:\n",
    "        numpy ndarray with shape (number of bankers, n_repeats, n_folds)\n",
    "        containing the utilities\n",
    "    \"\"\"\n",
    "    results = np.empty(shape=(len(bankers), n_repeats, n_folds))\n",
    "\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True)\n",
    "    for i in range(n_repeats):\n",
    "        j = 0\n",
    "        for train_indices, test_indices in kf.split(X):\n",
    "            X_train = X.iloc[train_indices, :]\n",
    "            X_test = X.iloc[test_indices, :]\n",
    "            y_train = y[train_indices]\n",
    "            y_test = y[test_indices]\n",
    "\n",
    "            for b, banker in enumerate(bankers):\n",
    "                banker.fit(X_train, y_train)\n",
    "\n",
    "                util, _ = utility_from_test_set(\n",
    "                    X_test, y_test, banker, interest_rate)\n",
    "                results[b, i, j] = util\n",
    "            j += 1\n",
    "    return results\n",
    "\n",
    "\n",
    "def compare_with_random(n_repeats, n_folds, response, interest_rate):\n",
    "    \"\"\" Tests the random banker against our group1 banker.\n",
    "\n",
    "    Args:\n",
    "        n_repeats: the number of repeated cv's\n",
    "        n_folds: number of folds in k-fold cv\n",
    "        response: the name of the response variable\n",
    "        interest_rate: float interest rate by month\n",
    "    \"\"\"\n",
    "\n",
    "    ## decision makers ##\n",
    "    # random banker\n",
    "    r_banker = random_banker.RandomBanker()\n",
    "    r_banker.set_interest_rate(interest_rate)\n",
    "\n",
    "    # group1 banker\n",
    "    g_banker = group1_banker.Group1Banker()\n",
    "    g_banker.set_interest_rate(interest_rate)\n",
    "\n",
    "    # get data\n",
    "    data = get_data()\n",
    "    # pop removes and returns the given column, \"response\" is no longer in data\n",
    "    y = data.pop(response)\n",
    "\n",
    "    return repeated_cross_validation_utility(\n",
    "        X=data, y=y,\n",
    "        bankers=[r_banker, g_banker],\n",
    "        interest_rate=interest_rate,\n",
    "        n_repeats=n_repeats, n_folds=n_folds\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 15min 30s, sys: 17min 27s, total: 32min 58s\nWall time: 4min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = 'repaid'\n",
    "results = compare_with_random(100, 5, response, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Average utilities\n-----------------\n Random banker: 568219.88\n Our banker:    1153313.934\n"
     ]
    }
   ],
   "source": [
    "print(\"Average utilities\\n-----------------\\n\",\n",
    "    f\"Random banker: {results[0].mean()}\\n\",\n",
    "    f\"Our banker:    {results[1].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Density')"
      ]
     },
     "metadata": {},
     "execution_count": 13
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"272.794688pt\" version=\"1.1\" viewBox=\"0 0 385.78125 272.794688\" width=\"385.78125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 272.794688 \nL 385.78125 272.794688 \nL 385.78125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 43.78125 235.238438 \nL 378.58125 235.238438 \nL 378.58125 17.798438 \nL 43.78125 17.798438 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 80.483344 235.238438 \nL 88.327974 235.238438 \nL 88.327974 229.564856 \nL 80.483344 229.564856 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 88.327974 235.238438 \nL 96.172604 235.238438 \nL 96.172604 209.707322 \nL 88.327974 209.707322 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 96.172604 235.238438 \nL 104.017234 235.238438 \nL 104.017234 187.012997 \nL 96.172604 187.012997 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 104.017234 235.238438 \nL 111.861865 235.238438 \nL 111.861865 158.645091 \nL 104.017234 158.645091 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 111.861865 235.238438 \nL 119.706495 235.238438 \nL 119.706495 42.336676 \nL 111.861865 42.336676 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 119.706495 235.238438 \nL 127.551125 235.238438 \nL 127.551125 65.031001 \nL 119.706495 65.031001 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 127.551125 235.238438 \nL 135.395755 235.238438 \nL 135.395755 28.152723 \nL 127.551125 28.152723 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 135.395755 235.238438 \nL 143.240385 235.238438 \nL 143.240385 45.173467 \nL 135.395755 45.173467 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 143.240385 235.238438 \nL 151.085015 235.238438 \nL 151.085015 104.74607 \nL 143.240385 104.74607 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 151.085015 235.238438 \nL 158.929646 235.238438 \nL 158.929646 130.277185 \nL 151.085015 130.277185 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_13\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 158.929646 235.238438 \nL 166.774276 235.238438 \nL 166.774276 116.093232 \nL 158.929646 116.093232 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 166.774276 235.238438 \nL 174.618906 235.238438 \nL 174.618906 189.849788 \nL 166.774276 189.849788 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 174.618906 235.238438 \nL 182.463536 235.238438 \nL 182.463536 206.870531 \nL 174.618906 206.870531 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 182.463536 235.238438 \nL 190.308166 235.238438 \nL 190.308166 198.36016 \nL 182.463536 198.36016 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_17\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 190.308166 235.238438 \nL 198.152796 235.238438 \nL 198.152796 223.891275 \nL 190.308166 223.891275 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 198.152796 235.238438 \nL 205.997426 235.238438 \nL 205.997426 223.891275 \nL 198.152796 223.891275 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 205.997426 235.238438 \nL 213.842057 235.238438 \nL 213.842057 221.054484 \nL 205.997426 221.054484 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 135.732175 235.238438 \nL 146.765464 235.238438 \nL 146.765464 223.136748 \nL 135.732175 223.136748 \nz\n\" style=\"fill:#ff7f0e;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 146.765464 235.238438 \nL 157.798753 235.238438 \nL 157.798753 217.085903 \nL 146.765464 217.085903 \nz\n\" style=\"fill:#ff7f0e;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_22\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 157.798753 235.238438 \nL 168.832042 235.238438 \nL 168.832042 172.713041 \nL 157.798753 172.713041 \nz\n\" style=\"fill:#ff7f0e;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_23\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 168.832042 235.238438 \nL 179.865331 235.238438 \nL 179.865331 160.611352 \nL 168.832042 160.611352 \nz\n\" style=\"fill:#ff7f0e;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_24\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 179.865331 235.238438 \nL 190.89862 235.238438 \nL 190.89862 116.23849 \nL 179.865331 116.23849 \nz\n\" style=\"fill:#ff7f0e;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_25\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 190.89862 235.238438 \nL 201.931909 235.238438 \nL 201.931909 75.899524 \nL 190.89862 75.899524 \nz\n\" style=\"fill:#ff7f0e;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_26\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 201.931909 235.238438 \nL 212.965198 235.238438 \nL 212.965198 142.458817 \nL 201.931909 142.458817 \nz\n\" style=\"fill:#ff7f0e;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_27\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 212.965198 235.238438 \nL 223.998487 235.238438 \nL 223.998487 110.187645 \nL 212.965198 110.187645 \nz\n\" style=\"fill:#ff7f0e;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_28\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 223.998487 235.238438 \nL 235.031776 235.238438 \nL 235.031776 142.458817 \nL 223.998487 142.458817 \nz\n\" style=\"fill:#ff7f0e;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_29\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 235.031776 235.238438 \nL 246.065065 235.238438 \nL 246.065065 118.255438 \nL 235.031776 118.255438 \nz\n\" style=\"fill:#ff7f0e;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_30\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 246.065065 235.238438 \nL 257.098354 235.238438 \nL 257.098354 176.746938 \nL 246.065065 176.746938 \nz\n\" style=\"fill:#ff7f0e;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_31\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 257.098354 235.238438 \nL 268.131643 235.238438 \nL 268.131643 188.848627 \nL 257.098354 188.848627 \nz\n\" style=\"fill:#ff7f0e;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_32\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 268.131643 235.238438 \nL 279.164932 235.238438 \nL 279.164932 219.102851 \nL 268.131643 219.102851 \nz\n\" style=\"fill:#ff7f0e;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_33\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 279.164932 235.238438 \nL 290.198221 235.238438 \nL 290.198221 231.204541 \nL 279.164932 231.204541 \nz\n\" style=\"fill:#ff7f0e;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_34\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 290.198221 235.238438 \nL 301.23151 235.238438 \nL 301.23151 231.204541 \nL 290.198221 231.204541 \nz\n\" style=\"fill:#ff7f0e;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_35\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 301.23151 235.238438 \nL 312.264799 235.238438 \nL 312.264799 233.221489 \nL 301.23151 233.221489 \nz\n\" style=\"fill:#ff7f0e;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_36\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 312.264799 235.238438 \nL 323.298088 235.238438 \nL 323.298088 233.221489 \nL 312.264799 233.221489 \nz\n\" style=\"fill:#ff7f0e;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_37\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 323.298088 235.238438 \nL 334.331377 235.238438 \nL 334.331377 233.221489 \nL 323.298088 233.221489 \nz\n\" style=\"fill:#ff7f0e;opacity:0.4;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m1724733782\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"66.287935\" xlink:href=\"#m1724733782\" y=\"235.238438\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0.0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g transform=\"translate(58.336373 249.836875)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"128.912713\" xlink:href=\"#m1724733782\" y=\"235.238438\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 0.5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(120.96115 249.836875)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"191.53749\" xlink:href=\"#m1724733782\" y=\"235.238438\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 1.0 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(183.585928 249.836875)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"254.162268\" xlink:href=\"#m1724733782\" y=\"235.238438\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 1.5 -->\n      <g transform=\"translate(246.210706 249.836875)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"316.787046\" xlink:href=\"#m1724733782\" y=\"235.238438\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 2.0 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(308.835483 249.836875)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- Average utility over different random train/test draws -->\n     <defs>\n      <path d=\"M 34.1875 63.1875 \nL 20.796875 26.90625 \nL 47.609375 26.90625 \nz\nM 28.609375 72.90625 \nL 39.796875 72.90625 \nL 67.578125 0 \nL 57.328125 0 \nL 50.6875 18.703125 \nL 17.828125 18.703125 \nL 11.1875 0 \nL 0.78125 0 \nz\n\" id=\"DejaVuSans-65\"/>\n      <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path d=\"M 37.109375 75.984375 \nL 37.109375 68.5 \nL 28.515625 68.5 \nQ 23.6875 68.5 21.796875 66.546875 \nQ 19.921875 64.59375 19.921875 59.515625 \nL 19.921875 54.6875 \nL 34.71875 54.6875 \nL 34.71875 47.703125 \nL 19.921875 47.703125 \nL 19.921875 0 \nL 10.890625 0 \nL 10.890625 47.703125 \nL 2.296875 47.703125 \nL 2.296875 54.6875 \nL 10.890625 54.6875 \nL 10.890625 58.5 \nQ 10.890625 67.625 15.140625 71.796875 \nQ 19.390625 75.984375 28.609375 75.984375 \nz\n\" id=\"DejaVuSans-102\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n      <path d=\"M 25.390625 72.90625 \nL 33.6875 72.90625 \nL 8.296875 -9.28125 \nL 0 -9.28125 \nz\n\" id=\"DejaVuSans-47\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      <path d=\"M 4.203125 54.6875 \nL 13.1875 54.6875 \nL 24.421875 12.015625 \nL 35.59375 54.6875 \nL 46.1875 54.6875 \nL 57.421875 12.015625 \nL 68.609375 54.6875 \nL 77.59375 54.6875 \nL 63.28125 0 \nL 52.6875 0 \nL 40.921875 44.828125 \nL 29.109375 0 \nL 18.5 0 \nz\n\" id=\"DejaVuSans-119\"/>\n     </defs>\n     <g transform=\"translate(77.135938 263.515)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"62.533203\" xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"121.712891\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"183.236328\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"224.349609\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"285.628906\" xlink:href=\"#DejaVuSans-103\"/>\n      <use x=\"349.105469\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"410.628906\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"442.416016\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"505.794922\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"545.003906\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"572.787109\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"600.570312\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"628.353516\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"667.5625\" xlink:href=\"#DejaVuSans-121\"/>\n      <use x=\"726.742188\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"758.529297\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"819.710938\" xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"878.890625\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"940.414062\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"981.527344\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1013.314453\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"1076.791016\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"1104.574219\" xlink:href=\"#DejaVuSans-102\"/>\n      <use x=\"1139.779297\" xlink:href=\"#DejaVuSans-102\"/>\n      <use x=\"1174.984375\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"1236.507812\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"1275.371094\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"1336.894531\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"1400.273438\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"1439.482422\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1471.269531\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"1512.382812\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"1573.662109\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"1637.041016\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"1700.517578\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"1761.699219\" xlink:href=\"#DejaVuSans-109\"/>\n      <use x=\"1859.111328\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1890.898438\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"1930.107422\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"1971.220703\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"2032.5\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"2060.283203\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"2123.662109\" xlink:href=\"#DejaVuSans-47\"/>\n      <use x=\"2157.353516\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"2196.5625\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"2258.085938\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"2310.185547\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"2349.394531\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"2381.181641\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"2444.658203\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"2485.771484\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"2547.050781\" xlink:href=\"#DejaVuSans-119\"/>\n      <use x=\"2628.837891\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- 1e6 -->\n     <defs>\n      <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n     </defs>\n     <g transform=\"translate(359.703125 262.515)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-49\"/>\n      <use x=\"63.623047\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"125.146484\" xlink:href=\"#DejaVuSans-54\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_6\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m7435af4687\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m7435af4687\" y=\"235.238438\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.0 -->\n      <g transform=\"translate(20.878125 239.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m7435af4687\" y=\"190.81997\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.5 -->\n      <g transform=\"translate(20.878125 194.619189)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m7435af4687\" y=\"146.401503\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 1.0 -->\n      <g transform=\"translate(20.878125 150.200722)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m7435af4687\" y=\"101.983036\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 1.5 -->\n      <g transform=\"translate(20.878125 105.782254)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m7435af4687\" y=\"57.564568\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 2.0 -->\n      <g transform=\"translate(20.878125 61.363787)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_13\">\n     <!-- Density -->\n     <defs>\n      <path d=\"M 19.671875 64.796875 \nL 19.671875 8.109375 \nL 31.59375 8.109375 \nQ 46.6875 8.109375 53.6875 14.9375 \nQ 60.6875 21.78125 60.6875 36.53125 \nQ 60.6875 51.171875 53.6875 57.984375 \nQ 46.6875 64.796875 31.59375 64.796875 \nz\nM 9.8125 72.90625 \nL 30.078125 72.90625 \nQ 51.265625 72.90625 61.171875 64.09375 \nQ 71.09375 55.28125 71.09375 36.53125 \nQ 71.09375 17.671875 61.125 8.828125 \nQ 51.171875 0 30.078125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-68\"/>\n     </defs>\n     <g transform=\"translate(14.798438 145.527031)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-68\"/>\n      <use x=\"77.001953\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"138.525391\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"201.904297\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"254.003906\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"281.787109\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"320.996094\" xlink:href=\"#DejaVuSans-121\"/>\n     </g>\n    </g>\n    <g id=\"text_14\">\n     <!-- 1e−6 -->\n     <defs>\n      <path d=\"M 10.59375 35.5 \nL 73.1875 35.5 \nL 73.1875 27.203125 \nL 10.59375 27.203125 \nz\n\" id=\"DejaVuSans-8722\"/>\n     </defs>\n     <g transform=\"translate(43.78125 14.798437)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-49\"/>\n      <use x=\"63.623047\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"125.146484\" xlink:href=\"#DejaVuSans-8722\"/>\n      <use x=\"208.935547\" xlink:href=\"#DejaVuSans-54\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_11\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 58.999432 235.210123 \nL 64.553024 235.106369 \nL 68.718217 234.788791 \nL 71.495013 234.344674 \nL 74.271809 233.603511 \nL 75.660207 233.078788 \nL 77.048605 232.419062 \nL 78.437003 231.592126 \nL 79.825401 230.559147 \nL 81.213799 229.276385 \nL 82.602197 227.699029 \nL 83.990595 225.787011 \nL 85.378992 223.511957 \nL 86.76739 220.863694 \nL 88.155788 217.854317 \nL 90.932584 210.904093 \nL 93.70938 203.04913 \nL 96.486176 194.483325 \nL 97.874574 189.830155 \nL 99.262972 184.782539 \nL 100.65137 179.18848 \nL 102.039768 172.890595 \nL 103.428166 165.756848 \nL 104.816563 157.711914 \nL 106.204961 148.763171 \nL 108.981757 128.673552 \nL 113.146951 97.173219 \nL 114.535349 87.655321 \nL 115.923747 79.110973 \nL 117.312145 71.705823 \nL 118.700543 65.499006 \nL 120.088941 60.44833 \nL 121.477339 56.431335 \nL 122.865737 53.277869 \nL 124.254134 50.807877 \nL 125.642532 48.867509 \nL 127.03093 47.357224 \nL 128.419328 46.247285 \nL 129.807726 45.578389 \nL 131.196124 45.447886 \nL 132.584522 45.984634 \nL 133.97292 47.31754 \nL 135.361318 49.543998 \nL 136.749716 52.704299 \nL 138.138114 56.766777 \nL 139.526512 61.62615 \nL 142.303308 73.023451 \nL 146.468501 91.110109 \nL 149.245297 101.834751 \nL 150.633695 106.563646 \nL 153.410491 114.868913 \nL 158.964083 129.754432 \nL 160.352481 133.911325 \nL 161.740879 138.481247 \nL 163.129276 143.513909 \nL 165.906072 154.841008 \nL 170.071266 172.902475 \nL 171.459664 178.415878 \nL 172.848062 183.369162 \nL 174.23646 187.652154 \nL 175.624858 191.225565 \nL 177.013256 194.122263 \nL 178.401654 196.437752 \nL 179.790052 198.31209 \nL 182.566847 201.380279 \nL 185.343643 204.4621 \nL 188.120439 208.117974 \nL 193.674031 216.173225 \nL 195.062429 217.921737 \nL 196.450827 219.446445 \nL 197.839225 220.726169 \nL 199.227623 221.764674 \nL 200.616021 222.587968 \nL 203.392816 223.77387 \nL 207.55801 225.247324 \nL 210.334806 226.549938 \nL 213.111602 228.221397 \nL 218.665194 231.828246 \nL 221.441989 233.228425 \nL 224.218785 234.187462 \nL 226.995581 234.752645 \nL 229.772377 235.040053 \nL 233.937571 235.19332 \nL 235.325969 235.207189 \nL 235.325969 235.207189 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_12\">\n    <path clip-path=\"url(#pddb0040ee9)\" d=\"M 106.700484 235.20876 \nL 112.76338 235.100557 \nL 116.805311 234.872178 \nL 120.847241 234.386219 \nL 122.868206 233.9987 \nL 124.889171 233.48731 \nL 126.910137 232.832283 \nL 128.931102 232.015829 \nL 130.952067 231.022736 \nL 132.973032 229.840422 \nL 134.993998 228.458385 \nL 137.014963 226.86715 \nL 139.035928 225.056925 \nL 141.056893 223.016303 \nL 143.077858 220.731452 \nL 145.098824 218.186264 \nL 147.119789 215.363815 \nL 149.140754 212.249162 \nL 151.161719 208.83303 \nL 153.182685 205.115405 \nL 155.20365 201.10781 \nL 159.24558 192.322655 \nL 163.287511 182.727252 \nL 167.329441 172.521796 \nL 171.371372 161.743676 \nL 175.413302 150.349613 \nL 183.497163 126.691695 \nL 185.518128 121.188211 \nL 187.539093 116.190942 \nL 189.560059 111.900117 \nL 191.581024 108.495599 \nL 193.601989 106.106304 \nL 195.622954 104.783089 \nL 197.64392 104.483003 \nL 199.664885 105.071105 \nL 201.68585 106.341625 \nL 203.706815 108.054453 \nL 209.769711 113.78173 \nL 213.811641 117.089505 \nL 221.895502 122.990931 \nL 227.958398 127.540491 \nL 229.979363 129.220691 \nL 232.000328 131.115991 \nL 234.021294 133.33956 \nL 236.042259 135.999399 \nL 238.063224 139.173027 \nL 240.084189 142.886808 \nL 242.105155 147.105904 \nL 244.12612 151.737671 \nL 254.230946 176.279236 \nL 258.272876 185.015681 \nL 262.314807 193.101659 \nL 266.356737 200.80799 \nL 270.398668 208.10249 \nL 272.419633 211.499092 \nL 274.440598 214.664362 \nL 276.461563 217.558524 \nL 278.482529 220.155193 \nL 280.503494 222.442486 \nL 282.524459 224.421436 \nL 284.545424 226.103303 \nL 286.56639 227.507036 \nL 288.587355 228.657422 \nL 290.60832 229.583745 \nL 292.629285 230.318442 \nL 294.65025 230.895394 \nL 298.692181 231.706206 \nL 302.734111 232.239862 \nL 310.817972 232.995569 \nL 320.922798 233.73696 \nL 326.985694 233.979189 \nL 339.111485 234.376705 \nL 353.258242 235.098593 \nL 361.342103 235.212457 \nL 363.363068 235.215137 \nL 363.363068 235.215137 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_38\">\n    <path d=\"M 43.78125 235.238438 \nL 43.78125 17.798437 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_39\">\n    <path d=\"M 378.58125 235.238438 \nL 378.58125 17.798437 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_40\">\n    <path d=\"M 43.78125 235.238438 \nL 378.58125 235.238438 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_41\">\n    <path d=\"M 43.78125 17.798438 \nL 378.58125 17.798438 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_42\">\n     <path d=\"M 260.5 55.154687 \nL 371.58125 55.154687 \nQ 373.58125 55.154687 373.58125 53.154687 \nL 373.58125 24.798437 \nQ 373.58125 22.798437 371.58125 22.798437 \nL 260.5 22.798437 \nQ 258.5 22.798437 258.5 24.798437 \nL 258.5 53.154687 \nQ 258.5 55.154687 260.5 55.154687 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_43\">\n     <path d=\"M 262.5 34.396875 \nL 282.5 34.396875 \nL 282.5 27.396875 \nL 262.5 27.396875 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n    </g>\n    <g id=\"text_15\">\n     <!-- Random banker -->\n     <defs>\n      <path d=\"M 44.390625 34.1875 \nQ 47.5625 33.109375 50.5625 29.59375 \nQ 53.5625 26.078125 56.59375 19.921875 \nL 66.609375 0 \nL 56 0 \nL 46.6875 18.703125 \nQ 43.0625 26.03125 39.671875 28.421875 \nQ 36.28125 30.8125 30.421875 30.8125 \nL 19.671875 30.8125 \nL 19.671875 0 \nL 9.8125 0 \nL 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.578125 72.90625 50.734375 67.671875 \nQ 56.890625 62.453125 56.890625 51.90625 \nQ 56.890625 45.015625 53.6875 40.46875 \nQ 50.484375 35.9375 44.390625 34.1875 \nz\nM 19.671875 64.796875 \nL 19.671875 38.921875 \nL 32.078125 38.921875 \nQ 39.203125 38.921875 42.84375 42.21875 \nQ 46.484375 45.515625 46.484375 51.90625 \nQ 46.484375 58.296875 42.84375 61.546875 \nQ 39.203125 64.796875 32.078125 64.796875 \nz\n\" id=\"DejaVuSans-82\"/>\n      <path d=\"M 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\nM 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nz\n\" id=\"DejaVuSans-98\"/>\n      <path d=\"M 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 31.109375 \nL 44.921875 54.6875 \nL 56.390625 54.6875 \nL 27.390625 29.109375 \nL 57.625 0 \nL 45.90625 0 \nL 18.109375 26.703125 \nL 18.109375 0 \nL 9.078125 0 \nz\n\" id=\"DejaVuSans-107\"/>\n     </defs>\n     <g transform=\"translate(290.5 34.396875)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-82\"/>\n      <use x=\"67.232422\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"128.511719\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"191.890625\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"255.367188\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"316.548828\" xlink:href=\"#DejaVuSans-109\"/>\n      <use x=\"413.960938\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"445.748047\" xlink:href=\"#DejaVuSans-98\"/>\n      <use x=\"509.224609\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"570.503906\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"633.882812\" xlink:href=\"#DejaVuSans-107\"/>\n      <use x=\"688.167969\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"749.691406\" xlink:href=\"#DejaVuSans-114\"/>\n     </g>\n    </g>\n    <g id=\"patch_44\">\n     <path d=\"M 262.5 49.075 \nL 282.5 49.075 \nL 282.5 42.075 \nL 262.5 42.075 \nz\n\" style=\"fill:#ff7f0e;opacity:0.4;\"/>\n    </g>\n    <g id=\"text_16\">\n     <!-- Group1 banker -->\n     <defs>\n      <path d=\"M 59.515625 10.40625 \nL 59.515625 29.984375 \nL 43.40625 29.984375 \nL 43.40625 38.09375 \nL 69.28125 38.09375 \nL 69.28125 6.78125 \nQ 63.578125 2.734375 56.6875 0.65625 \nQ 49.8125 -1.421875 42 -1.421875 \nQ 24.90625 -1.421875 15.25 8.5625 \nQ 5.609375 18.5625 5.609375 36.375 \nQ 5.609375 54.25 15.25 64.234375 \nQ 24.90625 74.21875 42 74.21875 \nQ 49.125 74.21875 55.546875 72.453125 \nQ 61.96875 70.703125 67.390625 67.28125 \nL 67.390625 56.78125 \nQ 61.921875 61.421875 55.765625 63.765625 \nQ 49.609375 66.109375 42.828125 66.109375 \nQ 29.4375 66.109375 22.71875 58.640625 \nQ 16.015625 51.171875 16.015625 36.375 \nQ 16.015625 21.625 22.71875 14.15625 \nQ 29.4375 6.6875 42.828125 6.6875 \nQ 48.046875 6.6875 52.140625 7.59375 \nQ 56.25 8.5 59.515625 10.40625 \nz\n\" id=\"DejaVuSans-71\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n     </defs>\n     <g transform=\"translate(290.5 49.075)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-71\"/>\n      <use x=\"77.490234\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"116.353516\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"177.535156\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"240.914062\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"304.390625\" xlink:href=\"#DejaVuSans-49\"/>\n      <use x=\"368.013672\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"399.800781\" xlink:href=\"#DejaVuSans-98\"/>\n      <use x=\"463.277344\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"524.556641\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"587.935547\" xlink:href=\"#DejaVuSans-107\"/>\n      <use x=\"642.220703\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"703.744141\" xlink:href=\"#DejaVuSans-114\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pddb0040ee9\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"17.798438\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "sns.distplot(results[0, :, :].flatten(), label=\"Random banker\")\n",
    "sns.distplot(results[1, :, :].flatten(), label=\"Group1 banker\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Average utility over different random train/test draws\")\n",
    "plt.ylabel(\"Density\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Based on 100 repeats of 5-fold cv, our model using logistic regression is considerably better than the random model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions: reproducibility, reliability and privacy\n",
    "\n",
    "### Is it possible to ensure that the policy maximises revenue?\n",
    "\n",
    "We cannot ensure that the policy maximises revenue. This is due to the fact that the model is not perfect and we are maximising the expected utility. There is an uncertainty in the estimated probability of a new individual being credit-worthy. If there were no uncertainty in this and we knew in advance which of the new individuals were going to pay back, we could ensure that a policy would maximise revenue by only lending to the credit-worthy individuals.\n",
    "\n",
    "### How can we take into account the uncertainty due to limited/biased data?\n",
    "\n",
    "In an ideal world we would have an independent replication of our study and check whether or not new, independent data would develop the same type of policy that we obtained with the current data set. Another option is simulation of data in order to validate our policy based on constructed data (Dimitrakakis, 2020, pp. 35-36). When it comes to the current policy, there are several ways of taking this uncertainty into account when considering our objective of maximising expected utility.\n",
    "\n",
    "### What are the consequences if the model is wrong?\n",
    "\n",
    "There will always be a possibility that the model performs a lot worse in practice. For example our data set could be a bad representation of the population because of some change in the population properties after it was collected. Therefore it is important not to put too much trust into new models. Simple interpretable models have a clear advantage in this regard. If the model is simple enough you might spot errors before the model is tested in practice, and if you find a model to be wrong, it will be easier to understand why.\n",
    "\n",
    "For the bank, one of the major consequences of the model being wrong is the loss of profit. If the model is granting credit too easily, the bank could incur losses due to the fact that they are granting credit and then loosing the entire investement $m$. If the model is too strict about granting credit, the bank is missing potential profit from the individuals that were declined credit, this would be $-m((1+r)^n - 1)$. The missed potential income could also cause the individuals that were wrongfully declined credit to apply for credit elsewhere. In that case, a hard-to-intepret model would also increase the difficulty in troubleshooting the reasons for the wrongfully decline of credit.\n",
    "\n",
    "### How can we take into account the risk of the model being wrong?\n",
    "\n",
    "It is possible to consider the two types of error the model can generate: \n",
    "\n",
    "1. classify new individuals as credit-worthy when they are in fact not ($a_{10}$)\n",
    "2. classify new individuals as not being credit-worthy when they in fact are ($a_{01}$)\n",
    "\n",
    "We can indicate the class of actual credit-worthy individuals as 'positive' ($a_{1}$) and the class of individuals not being credit-worthy as 'negatives' ($a_{0}$). Then our error (1) can be called a false positive and the other type of error (2) as a false negative. This corresponds to type 1 and type 2 erros, where the probability of type 1 errors is equivalent to the probability of false positive and type 2 error is equivalent to the probability of false negative. These probabilities can be estimated by looking at the fraction $\\frac{a_{10}}{\\# a_{1}}$. Similarily for the false negatives $\\frac{a_{01}}{\\# a_{0}}$. Adapted from (Azzalini & Scarpa, 2012, p. 139). For our example, if a new individual is classified as credit-worthy and this is a false positive, it implies the loss $-m$ (the lost investment). While if a new individual is classified as not being credit-worthy and this is a false negative, it implies the loss of $-m[(1+r)^{n} - 1]$ (the \"lost\" return on investment). \n",
    "\n",
    "The different actions are weighted by their reward values, so the decision maker grants credit based on the reward-weighted utility, not only whether or not the probability of repayment is above a threshold, such as $>0.5$. These weights are shown below:\n",
    "\n",
    "|   |           |            | True              |         |\n",
    "|---|-----------|------------|-------------------|---------|\n",
    "|   |           |            | No default        | Default |\n",
    "|   | Predicted | No default | $-m((1+r)^n - 1)$ | $-m$    |\n",
    "|   |           | Default    | 0                 | 0       |\n",
    "\n",
    "\n",
    "We could take into account the risk of the model being wrong by trying to control the type 1 error (false positive), we want to minimize false positives because this causes us to lose $-m$. We could enfore a higher threshold than 0 for the expected utility and then we could check this by demanding that the probability for type 1 errors (false positive) should be within an accepted range, e.g. maximum 5 %. We could attempt to find this threshold for type 1 erros by looking at the training data and check the probability for type 1 errors on a holdout set of the training data. Currently, we are attempting to control this by increasing the threshold for which to grant credit. When increasing the threshold for the expected utility, the model becomes more conservative. This would be equivalent to saying that we need to be for example $\\ge$ 75 % sure that the loan is going to be repaid rather than just $\\ge$ 50 % sure that the loan is going to be repaid in a situation where we only looked at the probability of repayment.\n",
    "\n",
    "We would want to have the percentage of false positives to be below a limit, e.g. $max\\_alpha = Pr(\\text{false positive}) = 0.05$ \n",
    "\n",
    "An outline of the algorithm:\n",
    "\n",
    "1. initialize $\\epsilon = 0$, $alpha\\_value = 1$ and $\\Delta \\epsilon = 1000$\n",
    "2. while $alpha\\_value \\ge max\\_alpha$:\n",
    "    * false_positives = 0\n",
    "    * predicted_actions = get_actions(validation_data, $\\epsilon$)\n",
    "    * false_positives += get_number_of_false_positives(predicted_actions, validation_data)\n",
    "    * $alpha\\_value$ = false_positives/len(validation_data)\n",
    "    * $\\epsilon = \\epsilon + \\Delta \\epsilon$\n",
    "3. alter policy so that $E(U(\\cdot)) > \\epsilon$ in order to grant credit to an individual \n",
    "\n",
    "\n",
    "\n",
    "### Does the existence of the database raise any privacy concerns?\n",
    "\n",
    "The database is anonymized because there are no directly identifying attributes about the individuals (Dimitrakakis, 2020, p. 74). However, there is very specific information about the individuals in the database, such as age, personal status, sex and information about the employement situation of the individuals. There is also information about the housing and property situation of the different individuals.\n",
    "\n",
    "There seems to be a high probability of inferring personal information about the individuals in the database by using for example record linkage. When considering differential privacy, it could be useful to assume that the adversary has potentially infinite side information (Dimitrakakis, 2020, p. 76). Where side information could be defined as information that an adversary has about all, except one observation of a dataset (A). We can call these datasets A and A’. The idea of differential privacy is then that even in the extreme case that the adversary has all this information, they should not be able to infer information about the missing observation based on differential private queries on A and A’ (Zhu, 2017, pp. 7-8). Unlimited access to this database could therefore be a large privacy concern. Both because adversaries could use side information to infer information from this database and/or because adversaries could use this database as side information to infer information from other databases. \n",
    "\n",
    "If we have the datasets A and A’ as above, an algorithm called ALG and define f(ALG(*)) as the probability distribution of the result. Then differential privacy could be defined as\n",
    "\n",
    "$$\n",
    "f(ALG(A)) < e^{\\epsilon}f(ALG(A’)) + \\delta\n",
    "$$\n",
    "\n",
    "where $\\delta = 0$ for $\\epsilon$-DP. The parameter $\\epsilon$ here controls the degree of equality between the distributions, adapted from (Le Ny, 2020, pp. 5-6). Based on this definition it would seem that by decreasing $\\epsilon$, the two distributions for the different algorithms would be more similar.\n",
    "\n",
    "### How would secret database and public credit decisions affect privacy?\n",
    "\n",
    "It would obviously be better to have the database secret instead of the database also being public. It could however be possible to infer a lot of information from the secret database by knowing the public credit decision. \n",
    "\n",
    "Because the bank is publishing the actual credit decisions, differential privacy is not possible. If, for example, an adversary with information about all but one attribute for a specific row, the adversary could infer the information of the missing attribute by using the public credit decisions. If the credit decisions were to be made public, the bank would have to consider the fact that adversaries could query the public “database” containing the credit decisions. This information could then be used as side information or together with additional side information to infer information about the individuals. \n",
    "\n",
    "One way to ensure privacy of the credit decision would be the “randomised response mechanism”. In this response mechanism, the true credit decision would be returned with probability p0 = 0.5 and with probability, p0’ = 1 – p0, the response mechanism would return 1 with probability 0.5 and 0 with probability 0.5. This calculation of the response could then be independent for each query (Dimitrakakis, 2020, p. 77). To ensure differential privacy for the public credit decisions, this algorithm could be implemented when potential adversaries queries the public “database”.\n",
    "\n",
    "### How can we protect the data of the people in the training set?\n",
    "\n",
    "We could use differential privacy to obscure the information in the public credit decisions that have been made by the bank.\n",
    "\n",
    "* if the database is secret, but the credit decisions are public (we assume that the identities of the individuals also are public): we would have a randomised response mechanism for the credit decision. This randomised response mechanism will ensure that the public responses are correct with a probability $\\le 1$ (Dimitrakakis, 2020, p. 77). This would make it harder to infer information about the individuals from the public information. Even if the database is not public, there is a risk of dishonest employees and/or digital attacks on the bank.\n",
    "\n",
    "The training data contains several columns of attributes. We could implement a mechanism where each of the attributes would be transformed independently and then the entire observation (with all attributes) could be returned (Dimitrakakis, 2020, p. 82). For categorical columns of the training data, it would be possible to implement a randomised response mechanism as above. For the numerical values one could use a Laplace mechanism in order to transform the response by adding Laplace-distributed noise to the returned value. This could also be done by adding Laplace-distributed noise to each variable before computing a response, which would be a local privacy model (Dimitrakakis, 2020, p. 83). In order to protect the training data w.r.t. privacy the following outline of an algorithm could be implemented\n",
    "\n",
    "1.\tAdd Laplace-noise to numerical attributes\n",
    "2.\tAdd randomized response mechanism to categorical attributes\n",
    "\n",
    "* if the database is public and the credit decisions are public: for categorical attributes, we could use a randomised response mechanism. For the numerical attributes, we can apply a Laplace mechanism in order to make the information differentially private. In this scenario as well, the bank is still vulnerable to dishonest employees and digital attacks as the original (true) data is still available inside the bank.\n",
    "\n",
    "One could also have performed randomized response mechanism for the credit decision of the training data if this was the only data that would be public (and would have needed protection). In that case one would have a situation more similar to a centralized privacy model when adversaries query the data. Because one could for example calculate the true number of accepted credit applications and then add noise before returning the response (Dimitrakakis, 2020, p. 83). This would protect the data of the people in the training set because an adversary would lose the ability to infer information with the credit decision as side-information.\n",
    "\n",
    "### How can we protect the data of the people that apply for loans?\n",
    "\n",
    "We assume that the data for a new individual could potentially be leaked and considered non-secret. In that case we could apply the same techniques as for people in the training set. We would then apply a random mechanism to the input data for the new applicant. This could be done in the same manner as for the training data. There would be two possible scenarios:\n",
    "\n",
    "1.\tProtect the data by adding Laplace-noise to numerical attributes and using the randomized response mechanism for the categorical attributes\n",
    "2.\tProtect the data by adding noise to the response variable after the policy has made the decision on the true data of the new test data. This would be a type of centralized privacy model as mentioned above where the noise is added after the calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a private decision making mechanism for people that apply for new loans\n",
    "\n",
    "We decided to implement local privacy with laplace noise for the quantitative attributes and randomized response mechanism for the categorical attributes. We then need to find values for the parameters of the noise. More specificly the scale $\\lambda$ for the laplace noise and the probability of changing a value $p$ for the randomised response mechanism. From (Dimitrakakis 2020, p. 84) we have that for a function $f$ with sensitivity $\\mathbb{L}(f)$:\n",
    "$$\n",
    "    \\epsilon \\ge \\mathbb{L}(f)/\\lambda\n",
    "$$\n",
    "Here the sensitivity is defined as $\\mathbb{L}(f) \\triangleq \\sup_{xNx'} |f(x) - f(x')|$, and can be estimated by the range of the attribute.\n",
    "\n",
    "From (Dimitrakakis 2020, p. 80) we have that for a randomized response mechanism with $p \\le 1/2$:\n",
    "$$\n",
    "    \\epsilon \\ge \\ln \\left( \\frac{1-p}{p} \\right)\n",
    "$$\n",
    "With $k$ as the number of attributes, to achieve $\\epsilon$-DP in total, each attribute needs to be $(\\epsilon/k)$-DP (Dimitrakakis 2020, p. 82). Using this and the equations above, we can find the noise parameters as a function of the total privacy guarantee $\\epsilon$:\n",
    "$$\n",
    "    \\lambda = \\frac{\\mathbb{L}(f) k}{\\epsilon}, \\quad p = \\frac{1}{e^{\\epsilon/k} + 1}\n",
    "$$\n",
    "\n",
    "To simulate people applying for new loans, we split the data into training and test sets, and only apply the noise to the covariates in the test set. The utility is estimated by 5-fold CV for a more stable result. The implementation is listed in the apendix under the file privacy_guarantee.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7fe0b19760f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mprivacy_guarantee\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutility_epsilons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mepsilon_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mutility\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutility_epsilons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon_sequence\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutility\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epsilon/k\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/IN-STK5000/ml-society-science/src/project-1/privacy_guarantee.py\u001b[0m in \u001b[0;36mutility_epsilons\u001b[0;34m(epsilon_sequence, verbose)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTestImplementation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mbanker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mpred_decision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbanker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             utility = TestImplementation.utility_from_obs(\n",
      "\u001b[0;32m~/Documents/IN-STK5000/ml-society-science/src/project-1/group1_banker.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utility_epsilon_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/IN-STK5000/ml-society-science/src/project-1/group1_banker.py\u001b[0m in \u001b[0;36m_fit_model\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \"\"\"\n\u001b[1;32m     46\u001b[0m         \u001b[0mlog_reg_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlog_reg_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0menable_utility_epsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m             \u001b[0mprefer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'processes'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[1;32m   1408\u001b[0m                                \u001b[0;34m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m             path_func(X, y, pos_class=class_, Cs=[C_],\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1027\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[1;32m    755\u001b[0m             iprint = [-1, 50, 1, 100, 101][\n\u001b[1;32m    756\u001b[0m                 np.searchsorted(np.array([0, 1, 2, 3]), verbose)]\n\u001b[0;32m--> 757\u001b[0;31m             opt_res = optimize.minimize(\n\u001b[0m\u001b[1;32m    758\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"L-BFGS-B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    607\u001b[0m                                   **options)\n\u001b[1;32m    608\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0m\u001b[1;32m    610\u001b[0m                                 callback=callback, **options)\n\u001b[1;32m    611\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_loss_and_grad\u001b[0;34m(w, X, y, alpha, sample_weight)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mz0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;31m# Case where we fit the intercept.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from privacy_guarantee import utility_epsilons\n",
    "epsilon_sequence = np.linspace(1, 400, 200)\n",
    "utility = utility_epsilons(epsilon_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(epsilon_sequence/24, utility)\n",
    "plt.xlabel(\"epsilon/k\")\n",
    "plt.ylabel(\"Total utility\")"
   ]
  },
  {
   "source": [
    "The plot above shows the cross validated total utility over different values of $\\epsilon/k$. The number of attributes $k$ is 24, so to get the total privacy guarantee one would have to multiply by 24."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate the amount of loss in utility as we change the privacy guarantee\n",
    "\n",
    "The amount of loss in utility is related to privacy. More privacy in general indicates less expected utility as more information is “lost” when the policy is deciding the best action. In order to estimate the loss in utility one can base the estimation of (alpha, beta) usefulness for an algorithm A, an algorithm with more privacy as A_hat and the (test) data set TD that is defined through Pr(|A(TD) – A_hat(TD)| <= alpha) > 1 – beta. A small difference between the algorithm and the privacy modified algorithm would imply a small loss in utility (Zhu, 2017, pp. 15-16). This would be one possible estimate of the loss in utility, while the absolute difference |U(TD) – U_hat(TD)| itself could be another estimate of loss in utility where U is the normal utility and U_hat is the utility with increased privacy.\n",
    "\n",
    "### How will the interest rate affect the decision maker(s)?\n",
    "\n",
    "The interest rate $r$ affects the action of the decision maker through the expected utility. The expected utility given that the credit application is granted, $a_{t}=1$ is defined as:\n",
    "\n",
    "$$\n",
    "E[U|a_{t} = 1] = -m \\cdot Pr(r = r_{0}|a_{t} = 1) + m((1 + r)^{n} - 1) \\cdot Pr(r = r_{1}|a_{t} = 1)\n",
    "$$\n",
    "\n",
    "This further implies that the interest rate only affects the reward where the debtor does not default. This reward defines the utility:\n",
    "\n",
    "$$\n",
    "m((1 + r)^{n} - 1)\n",
    "$$\n",
    "\n",
    "From this we see that increasing interest rate when investment $m$ and the number of periods $n \\ge 1$ are held constant will monotonically increase the expected utility with increasing interest rate. For the decision maker this would mean that the probability for repayment of the credit: $Pr(r = r_{1}|a_{t} = 1)$ could be lower and the decision maker would still accept the credit application.\n",
    "\n",
    "### How will the number of periods affect the decision maker(s)?\n",
    "\n",
    "For the number of periods, the situation is the same as above, the expected utility will increase monotonically with increasing $n$ as long as $r > 0$. This is also logical because the bank would increase its profit with the number of interest rate accruals from the loan.\n",
    "\n",
    "### What are two main ways to perturbate the data in order to protect the individuals of the data set?\n",
    "\n",
    "There seems to be two possible mechanism to use in order to protect the individuals of the datasets. These are input perturbation and output perturbation. The input perturbation would add differently distributed types of noise to the columns of the data in order to protect the private information. Output perturbation would use “secret” data in the algorithm and then add noise to the output of that algorithm (Le Ny, 2020, pp. 18-22). These ways correspond to adding noise directly to the columns of the data sets before predicting the best action as an input perturbation. If we would calculate the best action based on the private data and then obtain our “true” credit decision we could add noise to this output we would have a mechanism closer to the output perturbation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions: fairness\n",
    "\n",
    "In order to simplify notation, we define the following variables in the same manner as in (Dimitrakakis, 2020, pp. 103-104): \n",
    "* y: {1: the credit was repaid, 0: the credit was not repaid (default)}\n",
    "* a: {1: the application was accepted, 0: the application was rejected}\n",
    "* x: covariates\n",
    "* z: sensitive covariates\n",
    "* U(a, y): the decision maker’s utility function"
   ]
  },
  {
   "source": [
    "## Guiding questions\n",
    "\n",
    "### Identify sensitive variables\n",
    "\n",
    "There are several variables in the data set that could potentially be considered sensitive. Variables such as the 9th covariate “Personal status and sex” could be considered sensitive. Because ceteris paribus we would ideally have that the chance of being offered credit should be the same whether the applicant is e.g. a married male or a single female. \n",
    "Covariate 11 “present residence since” could also be considered sensitive, since we generally do not want to discriminate w.r.t. this because we cannot determine the reason for the change of residency.\n",
    "\n",
    "Covariate 13 “age” could also potentially be considered sensitive because, as with gender and personal status, we do not want to discriminate the credit decision upon age. Although we could argue that age must be considered when considering the credit decision w.r.t. repayment.\n",
    "\n",
    "Covariate 20 “foreign worker” ceteris paribus could also be discussed to be a sensitive variable. Whether or not the worker is foreign or not should ideally not affect the credit decision."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the original features already imply some bias in data collection?\n",
    "\n",
    "We can think of bias in the data collection as either what covariates where selected to be measured (bias in selecting the columns of the training set) and as what type of individuals were selected to be measured (bias in selecting the rows of the training set). The features of the data set could seem little fine grained because of the many categorical features that seems to have somewhat arbitrary thresholds for the categories. This is in the sense that there are for example categories for savings account with very different widths $(100, 400, 500 \\text{ and } [\\infty \\ge 1000])$. This categorical would e.g. place an individual with 1000 DM in the same category as an individual with an arbitrary large amount of savings.\n",
    "\n",
    "### Analysis of decision function\n",
    "\n",
    "It is important that the decision function does not discriminate against gender. To investigate how fair our model is with respect to gender, we plotted the amount of males/females that are classified as \"did repay\"/\"didnot repay\"\n",
    "\n",
    "![im](img/gender_compare.png)\n",
    "\n",
    "The first thing that is apparent in the plot is that our model overestimates how many that end up paying back. This is a problem, but it is not relevant to fairness.\n",
    "What is relevant to fairness is the proportions of males/females that are predicted to repay. In the true data the proportions of men and women seems aproximately equal within those who repaied and those who did not. This sugests that gender should not be very relevant in predicting wether someone repayes. That is however not the case in our predicted response. Our model greatly favors males even though the true data tells us that it shouldn't.\n",
    "\n",
    "### Fairness: balance of genders\n",
    "\n",
    "When it comes to balance w.r.t. gender as a concept of fairness. We can look at balance as the concept: if we knew the true response ($y$), the policy would have been independent w.r.t. to the action ($a$) when considering the sensitive variable (Dimitrakakis, 2020, p. 104). In our case, that would mean that the policy should have similar distributions for women and for men given that we know whether or not the individual repaid ($y$). This also makes sense intuitively as a concept of fairness. If the loan actually was repaid and all the other covariates held equal, gender should not influence the action of the policy.\n",
    "\n",
    "Formulated as an equation, this should approximately hold if the policy is balanced w.r.t. gender\n",
    "$$\n",
    "P^{\\pi}(a|y, z = \\text{male}) = P^{\\pi}(a|y, z = \\text{female})\n",
    "$$\n",
    "this should hold for y = 0/1 and z = male/female. Adapted from (Dimitrakakis, 2020, p. 105). In order to evaluate the fairness w.r.t. gender, we need to look at the output of the policy on the test set. Because our sensitive variable is gender and we want the action to be fair (independent on gender), we should separate the policy outcomes for the sensitive variables and then look at a and y based on this.\n",
    "\n",
    "![im](img/gender_balance.png)\n",
    "\n",
    "The histogram shows a graphical representation of the equation above that should hold approximately for balanced policies. The label 'predicted' in the histogram is equivalent to $a$ in the equation while 'true' corresponds to $y$.\n",
    "\n",
    "We can compare $P^{\\pi}(a| y, z = \\text{male})$ with $P^{\\pi}(a| y, z = \\text{female})$ by looking at the proportions of $a$ for different values of $y$ and $z$. Looking at these proportions we get:  \n",
    "\n",
    "(a=1, a=0)\n",
    "\n",
    "|                 | Male (z=1)        | Female (z=0)    |\n",
    "|-----------------|-------------------|-----------------|\n",
    "| No default (y=1)| (0.9286, 0.0714)  | (0.9, 0.1)      |\n",
    "| Default (y=0)   | (0.8936, 0.1064)  | (0.6667, 0.3332)|\n",
    "|                 |                   |                 |\n",
    "\n",
    "From the table, we can see that for the different genders, the proportion of $a=1$ was approximately equal when $y=1$, but for $y=0$ it seems like the proportion for $a=1$ is lower for women than for men.  This would imply $P^{\\pi}(a = 1| y = 0, z = \\text{male}) > P^{\\pi}(a = 1| y = 0, z = \\text{female})$, that is, men has a higher probability of getting accepted for credit than women when we look at those who did not repay. This again would imply that the policy is not balanced based on the equation above because it does not (approximately) hold for $y=0$.\n",
    "Consider relative frequency as a simplification of the probability distribution. The probability of event X can then be estimated with $\\frac{n(X)}{n}$ where n is the total number of observation and $n(X)$ is the number of $X$ (Devore & Berk, 2012, p. 58). We can then look at the total variation distance which is defined as\n",
    "$$\n",
    "\\frac{1}{2} \\sum_{a \\in A} |P(a|y, z=\\text{male}) - P(a|y, z=\\text{female})| \n",
    "$$\n",
    "adapted from (Wikipedia, 2020). We can then use the estimated relative frequencies in the table above to estimate the total variation distance, we would then have the total variation distance for the “probability distribution” of the decisions $a$ that were made when the individuals repaid $y=1$\n",
    "\n",
    "$$\n",
    "\\frac{1}{2}(|P(a=0|y=1, z=\\text{male}) - P(a=0|y=1, z=\\text{female})| + |P(a=1|y=1, z=\\text{male}) - P(a=1|y=1, z=\\text{female})|)\n",
    "$$\n",
    "And the total variation distance when the individuals did not repay ($y=0$)\n",
    "$$\n",
    "\\frac{1}{2}(|P(a=0|y=0, z=\\text{male}) - P(a=0|y=0, z=\\text{female})| + |P(a=1|y=0, z=\\text{male}) - P(a=1|y=0, z=\\text{female})|)\n",
    "$$\n",
    "thus looking at the estimated probability distribution of the decision $a$ in the cases $y=1$ and $y=0$.\n",
    "\n",
    "From these approximations to the total variation distance, we see that the total variation distance for the case $y=1$ is relatively small compared to the case when $y=0$. From this, we can see that for outcome $y=0$, the total variation distance is greater than for the outcome $y=1$. This reflect the histogram above and states that the “distribution” among those who did not repay ($y=0$) has more variation between the genders than the “distribution” among those who did repay ($y=1$).\n",
    "\n",
    "\n",
    "### What would happen if fairness also would consider the amount of loan requested?\n",
    "\n",
    "We could look at fairness when taking the amount of loan requested into account by checking the sensitive variable $z$, which in our case is gender of the individual applying for a loan. We would like to check the fairness metric \n",
    "$$\n",
    "F(\\theta, \\pi) = \\sum_{y, z, a} (P_{\\theta}^{\\pi}(a|z, y) - P_{\\theta}^{\\pi}(a|y))^{2}\n",
    "$$\n",
    "Adapted from (Dimitrakakis, 2020, p. 107). We simplify and use relative frequency for the different measures of probability in the metric above. From the metric, the unfairness in balance is 0 when the two genders have equal probability of being accepted credit $a=1$ and being denied credit $a=0$ given the different values of the true response $y$.\n",
    "We see from the data that the median amount in the dataset was 2319.5, we therefore make a threshold for this value and check the fairness metric for amounts larger than this and lower than this. We use 10 repeated 5-fold cross validation in order to ensure stable values that do not depend to much on randomness in the data. We see that the $F(\\theta, \\pi) \\approx 0.0699$, while for amounts below the median, $ F(\\theta, \\pi) \\approx 0.0909$ and for amounts above the median $ F(\\theta, \\pi) \\approx 0.0969$. This implies that the degree of unfairness has increased when taking the amount requested into account when calculating fairness.\n",
    "\n",
    "We also checked the gender ratios among the 10 % largest amounts when performing repeated cross validation. We then check the ratio of male/females being granted credit ($a=1$) among the largest amount of loans. This could give an indication of why there is more unfairness according to the metric above when it comes to the loans with the largest amounts. This is done by calculating the number of granted applications for males and females among the largest 10 % of the loan applications. This is calculated in the method ‘calculate_balance_ratios’.\n",
    "\n",
    "\n",
    "### Stochastic gradient descent to find a policy that balances out fairness and utility\n",
    "\n",
    "First, looking at “how much” the policy breaches the balance criterion can be defined as \n",
    "$$\n",
    "F(\\theta, \\pi) = \\sum_{a, y, z} |P_{\\theta}^{\\pi}(a|y, z = \\text{male}) - P_{\\theta}^{\\pi}(a|y, z=\\text{female})|^{2}\n",
    "$$\n",
    "and then looking at the general utility $U(\\theta, \\pi) = P^{\\pi}_{\\theta}(y = a)$, that is, the distribution of correctly classified individuals given the $\\theta$ and the policy $\\pi$. The two are then combined in order to define the “value” of a policy \n",
    "\n",
    "$$\n",
    "V(\\lambda, \\theta, \\pi) = (1-\\lambda)U(\\theta, \\pi) - \\lambda F(\\theta, \\pi)\n",
    "$$\n",
    "This derivation is adapted from (Dimitrakakis, 2020, pp. 107-108). From the definition above, we see that this value requires $\\theta$ and this is unknown.\n",
    "\n",
    "We can then define the expected value of the policy\n",
    "$$\n",
    "V(\\lambda, \\xi, \\pi) = \\int [(1-\\lambda)U(\\theta, \\pi) - \\lambda F(\\theta, \\pi)] d\\xi (\\theta)\n",
    "$$\n",
    "Adapted from from. The subjective distribution $\\xi$ could be estimated by a prior and a likelihood (Dimitrakakis, 2020, pp. 108-109). We see that this the integral that has to be solved over the different subjective beliefs in order to find the expected value of the policy $\\pi$ for a specific $\\lambda$.  \n",
    "\n",
    "#### Finding the posterior distribution of $\\theta$\n",
    "First, we need to define the posterior distribution for $\\theta$ so we can sample from it. Because we know that the values of the response $y \\in \\{0,1\\}$, we see that if we define $p*=\\frac{e^{\\beta_{0} + \\vec{\\beta}^{T} \\cdot \\vec{x}}}{1 + e^{\\beta_{0} + \\vec{\\beta}^{T} \\cdot \\vec{x}}}$, the probability for $y=1$ with the logistic regression, this corresponds to each $y$ being Bernoulli distributed. We also assume the $y$s are independent so that the likelihood is the product of the individual likelihoods. \n",
    "We also assume that the $y$s are independent which implies $p(y_{1}, … , y_{n}) = p(y_{1})…p(y_{n})$ (Devore & Berk, 2012, p. 354). This gives the likelihood function\n",
    "$$\n",
    "p(\\vec{y} |\\vec{\\beta}) = \\prod_{i=1}^{n} p*^{y_{i}}(1-p*)^{1-y_{i}}\n",
    "$$\n",
    "Which becomes\n",
    "$$\n",
    "log(p(\\vec{y} |\\vec{\\beta})) = \\sum_{i=1}^{n} y_{i}log(p*) + (1-y_{i})log(1-p*)\n",
    "$$\n",
    "\n",
    "In our case $\\theta = \\vec{\\beta}$ because we are using a logistic regression as a model. We therefore use the Bayesian approach\n",
    "$$\n",
    "p(\\vec{\\beta}) \\propto p(\\vec{y} | \\vec{\\beta})p(\\vec{\\beta})\n",
    "$$\n",
    "adapted from (Gelman et al., 2014, p. 63). We assume that the priors are normally distributed to begin with, that is $ p(\\vec{\\beta}) \\propto 1$. This gives that the posterior joint distribution is\n",
    "$$\n",
    "p(\\vec{\\beta}) \\propto p(\\vec{y} |\\vec{\\beta}) = \\prod_{i=1}^{n} p*^{y_{i}}(1-p*)^{1-y_{i}}\n",
    "$$\n",
    "We are then able to sample from the joint posterior distribution of $\\theta$.\n",
    "\n",
    "#### Policy $\\pi$\n",
    "The policy $\\pi (a|x)$ can be parametrized as a softmax policy to give \n",
    "$$\n",
    "\\pi_{\\beta} = \\frac{e^{\\beta^{T}x}}{1+ e^{\\beta^{T}x}}\n",
    "$$\n",
    "Together with another form of the expected value of the policy, we can define that the following integral should be maximized w.r.t. $\\pi_{\\beta}$\n",
    "$$\n",
    "\\int [(1-\\lambda)E_{\\theta}^{\\pi_\\beta}[U(\\theta, \\pi_{\\beta}) - \\lambda F(\\theta, \\pi_{\\beta})] d\\xi (\\theta)\n",
    "$$\n",
    "Adapted from (Dimitrakakis et al., 2017). For $\\theta$ values sampled from the $\\xi$ distribution, we would like to maximize this expected utility for the policy. Because $\\theta$ is present in the equation through $\\pi_{\\theta}$ we need the gradient of this policy. In general we could write\n",
    "$$\n",
    "\\nabla_{\\pi} V(\\lambda, \\xi, \\pi) = \\int [(1-\\lambda) \\nabla E_{\\theta}^{\\pi_\\beta}[U(\\theta, \\pi_{\\beta}) - \\lambda \\nabla F(\\theta, \\pi_{\\beta})] d\\xi (\\theta)\n",
    "$$\n",
    "This can be maximized for a given $\\lambda$ which is a parameter that balances the amount of utility versus the degree of fairness. We can also look at the gradient ascent for both the gained utility and the imbalance of fairness (cost). \n",
    "This can be summarized as \n",
    "$$\n",
    "\\sum_{x,y} \\sum_{a} \\pi_{\\theta}(a|x) U(a, y) - \\sum_{x,y} \\sum_{a} \\pi_{\\theta}(a|x) F(\\theta, \\pi_{\\beta})\n",
    "$$\n",
    "\n",
    "As the total quantity that we would like to maximize, given a $\\lambda$ (Dimitrakakis, 2020, pp. 65). When taking the gradient of the different parts in the equation above, we can split it into the utility part and the “deviance from fairness” part.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss in fairness, $\\nabla F(\\theta, \\pi)$\n",
    "We then get\n",
    "$$\n",
    "\\nabla \\sum_{a, y, z} (P_{\\theta}^{\\pi} (a|y, z) - P_{\\theta}^{\\pi} (a|y, z’))^{2} \\\\\n",
    "\\sum_{a, y, z} \\nabla (P_{\\theta}^{\\pi} (a|y, z) - P_{\\theta}^{\\pi} (a|y, z’))^{2} \\\\\n",
    "\\sum_{a, y, z} 2(P_{\\theta}^{\\pi} (a|y, z) - P_{\\theta}^{\\pi} (a|y, z’))(\\nabla P - \\nabla P’)\n",
    "$$\n",
    "We also use that \n",
    "$$\n",
    "P_{\\theta}^{\\pi} = \\sum_{x} P_{\\theta} (y|a, x, z) P(x|a, z) \\\\\n",
    "P_{\\theta}^{\\pi} = \\sum_{x} P_{\\theta} (y|a, x, z) \\frac{P(a, x, z)}{P(a, z)} \\\\\n",
    "P_{\\theta}^{\\pi} = \\sum_{x} P_{\\theta} (y|a, x, z) \\frac{P(a|x, z)P(x, z)}{P(a|z)P(z)} \\\\\n",
    "P_{\\theta}^{\\pi} = \\sum_{x} P_{\\theta} (y|a, x, z) \\frac{P(a|x, z)P(x|z)P(z)}{P(a|z)P(z)} \\\\\n",
    "P_{\\theta}^{\\pi} = \\sum_{x} P_{\\theta} (y|a, x, z) \\frac{P(a|x, z)P(x|z)}{P(a|z)} \\\\\n",
    "P_{\\theta}^{\\pi} = \\sum_{x} P_{\\theta} (y|a, x, z) P(x|z) \\frac{\\pi(a|x, z)}{\\pi(a|z)} \\\\\n",
    "$$\n",
    "Adapted from (University of Oslo, 2020). And then looking at $\\nabla F(\\theta, \\pi)$\n",
    "$$\n",
    "\\nabla F(\\theta, \\pi) = \\sum_{a, y, z} 2(P_{\\theta}^{\\pi} (a|y, z) - P_{\\theta}^{\\pi} (a|y, z’)) ( \\sum_{x} P_{\\theta} (y|a, x, z) P(x|z) \\nabla \\frac{\\pi(a|x, z)}{\\pi(a|z)} - \\sum_{x} P’_{\\theta} (y|a, x, z) P(x|z) \\nabla \\frac{\\pi(a|x, z’)}{\\pi(a|z’)})\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient of utility, $\\nabla U(\\theta, \\pi)$\n",
    "For the utility, this can be expressed as a gradient of the utility of all observations, if we use the simplification\n",
    "$$\n",
    "U(\\theta, \\pi) = \\sum_{x, y} \\sum_{a} U(a, x, y) \\pi_{\\beta} (a|x)\n",
    "$$\n",
    "We then have\n",
    "$$\n",
    "\\nabla U(\\theta, \\pi) = \\sum_{x, y} \\sum_{a} U(a, x, y) \\nabla \\pi_{\\beta} (a|x)\n",
    "$$\n",
    "Adapted from (Dimitrakakis, 2020, p. 65). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient of policy\n",
    "We see from the expressions above that we need the gradient of the policy in order to continue. We then look at the scenario where the data used in the model is regarded as a constant. We can then find the components of the gradient by normal partial derivatives of the policy w.r.t. the different $\\beta$ variables. In general, we then get \n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\beta_{i}} \\pi_{\\beta} = \\frac{x_{i} \\cdot (e^{\\beta^{T}x} + 1) - e^{\\beta^{T}x} x_{i} e^{\\beta^{T}x}}{(e^{\\beta^{T}x} + 1)^{2}} \\\\\n",
    "= \\frac{ x_{i} e^{\\beta^{T}x}}{(e^{\\beta^{T}x} + 1)^{2}}\n",
    "$$\n",
    "The cross-entropy loss function can then be defined\n",
    "$$\n",
    "L = -(y log(\\pi_{\\beta}) + (1-y) log(1 - \\pi_{\\beta}))\n",
    "$$\n",
    "Adapted from (Hastie et al., 2016, p. 309). When using p* given the model trained on the existing data we want to use stochastic gradient descent to optimize policy $\\pi_{\\beta}$ for all the training observations by varying the $\\beta$ value sin the policy. In order to minimize the loss $L$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\beta_{i}}  = -(\\frac{y}{\\pi_{\\beta}} \\cdot \\frac{\\partial \\pi_{\\beta}}{\\partial \\beta_{i}} – (1-y) \\frac{1}{1 - \\pi_{\\beta}} \\cdot \\frac{\\partial \\pi_{\\beta}}{\\partial \\beta_{i}}\n",
    "$$\n",
    "We could then, in theory use this loss together with the loss function and stochastic gradient descent in order to find a policy $\\pi_{\\beta}$ that maximizes the balanced value. As mentioned in (Dimitrakakis et al., 2017) this involves maximizing the value policy $\\pi_{\\beta}$ over the data using the integral shown above. When looking at the $\\lambda$ we see that setting this to 0 would yield the integral over utility and we would ignore loss in fairness $F$. In that sense we would have to fix the $\\lambda$ and then attempt to find the optimal policy $\\pi$ that adheres to the given balance between utility and loss of fairness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing questions\n",
    "\n",
    "## How can we measure whether our policy is fair?\n",
    "\n",
    "Measuring fairness is a complex process when it comes to policies within machine learning. We could both look at calibration (probability distribution for outcome $y$ conditional on the action $a$ should be similar for all different values of the sensitive variable $z$) and balance (probability distribution for action $a$ should be similar conditional on outcome $y$ for all sensitive variables $z$). These measures both try to describe the fairness of the policy (Dimitrakakis, 2020, p. 105). Other ways to consider whether or not the policy is fair is by looking at confusion matrices that describe the false positive rates as discussed above.\n",
    "\n",
    "## How does the training data affect the fairness of the policy?\n",
    "\n",
    "The training data affects the fairness of the policy through the fact that the model in the policy is fitted using the training data. The training data is also affected by the collection methods for the data. The model is then implicitly affecting the policy through the expected utility for the new observations that is considered for loans.\n",
    "We can for example not say anything detailed about how the training data was collected, both methodology for collecting the data and selection of what data to collect. The data could be biased in the sense that the bank could be collecting data only about those observations they previously have provided a loan. If this is how the data was collected, this “prefiltering” of the data makes it biased towards the applicants that repays because the observations that the bank considered too high risk to accept the credit application have already removed from the training data. \n",
    "\n",
    "## Summarizing comments\n",
    "The policy developed is as shown in the first part able to generate a higher expected utility than a random decision policy. The policy has also been modified in accordance with a local privacy model in order to increase privacy both for the training data and the test data. As was discussed in lecture 22.10.2020, the different ways of increasing the degree of privacy in the data depends on the future purpose of the data. For data that is to be published publicly, a local privacy model can make sense, but for data that is “protected” a more centralized approach to privacy can be applied. Applying privacy for example at the probability provided for $y=1$ by the model in the policy can be seen as protecting the data with regards to privacy while not distorting the data as much as a local privacy model (University of Oslo, 2020a). With regard to fairness, the balance between genders should in theory be improved if the approach with stochastic gradient descent from the last part of the report is performed. The current fairness w.r.t. balance could be seen to deviate, especially for the case when $y=0$, that is, the borrower defaulted. However, this aspect of fairness should improve after adjusting the policy with different SGD.\n",
    "\n",
    "Further, the deficiencies with the conclusions in the report are addressed in the different sections. However, the bias in data collection could be highlighted as problematic. Also, the imbalance of the different categories could be seen as problematic. There are also latent hyperpriors that could be approximated in when making these decisions, especially macroeconomic conditions that would collectively affect all borrowers in the same manner as mentioned in the lecture (University of Oslo, 2020a). The assumption that the entire loan is to be repaid to a constant interest rate is also problematic. Firstly, the interest rate often varies and secondly, borrowers often transfer the loan to other banks. When it comes to methodology, more statistical analysis should ideally be performed in order to increase the accuracy of the prediction model, such as variable selection and/or model selection.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Azzalini, A. & Scarpa, B. (2012). Data analysis and data mining: An introduction. Oxford: Oxford University Press.\n",
    "\n",
    "Devore, J. & Berk, L. (2012). Modern Mathematical Statistics with Applications\n",
    "(Springer Texts in Statistics). New York, NY: Springer New York.\n",
    "\n",
    "Dimitrakakis, C. (2020). *Machine learning in science and society*. Unpublished. Department of Informatics, University of Oslo.\n",
    "\n",
    "Dimitrakakis, C., Liu, Y., Parkes, D., & Radanovic, G. (2017). Bayesian fairness.\n",
    "\n",
    "Gelman, A., Carlin, J., Stern, H., Dunson, D., Vehtari, A. & Rubin, D. (2014). Bayesian data analysis (3rd ed., Texts in statistical science). Boca Raton, Fl: CRC Press.\n",
    "\n",
    "Hastie, T., Tibshirani, R. & Friedman, J. (2016). The Elements of Statistical Learning. Data Mining, Inference and Prediction. New York, NY: Springer New York.\n",
    "\n",
    "Le Ny, J. (2020). Differential privacy for dynamic data (1st ed. 2020, SpringerBriefs in electrical and computer engineering). Cham, Switzerland: Springer.\n",
    "\n",
    "University of Oslo. (2020). Tutlrial 2: Decisions, Utility and Fairness. Retrieved from https://www.uio.no/studier/emner/matnat/ifi/IN-STK5000/h20/forelesningsvideoer/2-tutorial-decisions-utility-fairness.mp4?vrtx=view-as-webpage \n",
    "\n",
    "University of Oslo. (2020a). Lecture videos. Retrieved from https://www.uio.no/studier/emner/matnat/ifi/IN-STK5000/h20/forelesningsvideoer/ \n",
    "\n",
    "Wikipedia. (2020, 12. august). Total variation distance of probability measures. Retrieved from https://en.wikipedia.orgwikiTotal_variation_distance_of_probability_measures \n",
    "\n",
    "Zhu, T. (2017). Differential privacy and applications (1st ed 2017 ed., Vol. 69, Advances in information security). Cham, Switzerland: Springer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "## Differential privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-1-a5dc9a9090b1>, line 74)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-a5dc9a9090b1>\"\u001b[0;36m, line \u001b[0;32m74\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def transform_categorical(data, p):\n",
    "    \"\"\" Transform a column of categorical data data with a randomised response mechanism\n",
    "\n",
    "    Args:\n",
    "        data: Array with data from a categorical attribute.\n",
    "        p: The probablity of changing a datapoint.\n",
    "\n",
    "    Returns:\n",
    "        Array of the same length as the input data, containing the transformed data.\n",
    "    \"\"\"\n",
    "    transform_indexing = rnd.choice([True, False], size=data.size, p=[p, 1-p])\n",
    "    new_values = rnd.choice(np.unique(data), size=transform_indexing.sum())\n",
    "    new_data = data.copy()\n",
    "    new_data[transform_indexing] = new_values\n",
    "    return new_data\n",
    "\n",
    "\n",
    "def transform_quantitative(data, b, scale_noise=False):\n",
    "    \"\"\" Transform a column of quantitative data with laplace noise\n",
    "\n",
    "    Args:\n",
    "        data: Array with the data from a quantitative attribute.\n",
    "        b: Positive float used as the second parameter of the laplace distribution,\n",
    "            referred to as the scale or the diversity of the distribution.\n",
    "        scale_noise: If true, scale the laplace noise by the standard deviation of the data.\n",
    "            This allows the same value for b to be used on differently scaled data\n",
    "\n",
    "    Returns:\n",
    "        Array of the same length as the input data, containing the transformed data.\n",
    "    \"\"\"\n",
    "    noise = rnd.laplace(0, b, size=data.size)\n",
    "    if scale_noise:\n",
    "        noise *= data.std()\n",
    "    return data + noise\n",
    "\n",
    "\n",
    "def apply_random_mechanism_to_data(data_frame, quantitative_names, categorical_names, laplace_delta, p):\n",
    "    \"\"\" Aplies a random mechanism to certain columns of a data frame\n",
    "\n",
    "    Args:\n",
    "        data_frame: A pandas data frame\n",
    "        quantitative_names: An iterable with the column names of the quantitative attributes\n",
    "            you wish to add laplace noise to.\n",
    "        categorical_names: An iterable with the column names of the categorical attributes you\n",
    "            wish to transform.\n",
    "        laplace_delta: The delta parameter to supply to the laplace noise\n",
    "        p: The probability to suppøy to the random noise for categorical data.\n",
    "\n",
    "    Returns:\n",
    "        Pandas data frame of the same dimentions as the one supplied, but with differentially private data.\n",
    "    \"\"\"\n",
    "    dp_data = data_frame.copy()\n",
    "\n",
    "    for column_name in quantitative_names:\n",
    "        dp_data[column_name] = transform_quantitative(\n",
    "            data_frame[column_name], b=laplace_delta, scale_noise=True)\n",
    "\n",
    "    for column_name in categorical_names:\n",
    "        dp_data[column_name] = transform_categorical(\n",
    "            data_frame[column_name], p)\n",
    "\n",
    "    return dp_data\n",
    "\n",
    "\n",
    "    def get_differentially_private_data(laplace_lambda, p):\n",
    "    \"\"\" Reads in the german data and applies a random mechanism\n",
    "\n",
    "    Args:\n",
    "        laplace_lambda: the lambda value to use in the laplace noise\n",
    "        p: the probability of changing a categorical value\n",
    "\n",
    "    Returns:\n",
    "        Differentially private data set.\n",
    "    \"\"\"\n",
    "    features = ['checking account balance', 'duration', 'credit history',\n",
    "                'purpose', 'amount', 'savings', 'employment', 'installment',\n",
    "                'marital status', 'other debtors', 'residence time',\n",
    "                'property', 'age', 'other installments', 'housing', 'credits',\n",
    "                'job', 'persons', 'phone', 'foreign', 'repaid']\n",
    "\n",
    "    data_raw = pd.read_csv(\"german.data\",\n",
    "                           delim_whitespace=True,\n",
    "                           names=features)\n",
    "\n",
    "    numeric_variables = ['duration', 'age', 'residence time', 'installment',\n",
    "                         'amount', 'persons', 'credits']\n",
    "    categorical_variables = set(features).difference(set(numeric_variables))\n",
    "\n",
    "    data_raw = differential_privacy.apply_random_mechanism_to_data(\n",
    "        data_raw, numeric_variables, categorical_variables, 0.3, 0.4)\n",
    "\n",
    "    data = pd.DataFrame(columns=numeric_variables)\n",
    "    data[numeric_variables] = data_raw[numeric_variables]\n",
    "\n",
    "    # Mapping the response to 0 and 1\n",
    "    data[\"repaid\"] = data_raw[\"repaid\"].map({1: 1, 2: 0})\n",
    "    # Create dummy variables for all the catagorical variables\n",
    "    not_dummy_names = numeric_variables + [\"repaid\"]\n",
    "    dummy_names = [x not in not_dummy_names for x in features]\n",
    "    dummies = pd.get_dummies(data_raw.iloc[:, dummy_names], drop_first=True)\n",
    "    data = data.join(dummies)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "    def compare_privacy_garantees(laplace_lambdas, p, n_repeats, n_folds, response, interest_rate):\n",
    "    \"\"\" Compare utility of models with differnt privacy guarantees.\n",
    "\n",
    "    Args:\n",
    "        laplace_lambdas: iterable with the different lambda values to use\n",
    "        p: probability of changing a categorical variable\n",
    "        n_repeats: number of repeats in the repeated cross validation\n",
    "        n_folds: number of folds in k-fold cv\n",
    "        response: the name of the response variable\n",
    "        interest_rate: the interest rate by month to use when calculating utility\n",
    "\n",
    "    Returns:\n",
    "        Dictionary on the form {string: numpy.ndarray(shape=(nrepeats, n_folds))}\n",
    "        with the results.\n",
    "    \"\"\"\n",
    "    g_banker = group1_banker.Group1Banker()\n",
    "    g_banker.set_interest_rate(interest_rate)\n",
    "\n",
    "    data_frames = []\n",
    "    data_frames.append(get_data())\n",
    "    for laplace_lambda in laplace_lambdas:\n",
    "        data_frames.append(get_differentially_private_data(laplace_lambda, p))\n",
    "\n",
    "    results = {}\n",
    "    for i, data_frame in enumerate(data_frames):\n",
    "        y = data_frame.pop(response)\n",
    "\n",
    "        new_result = repeated_cross_validation_utility(\n",
    "            X=data_frame, y=y,\n",
    "            bankers=[g_banker],\n",
    "            banker_names=[f\"lambda{i}\"],\n",
    "            interest_rate=interest_rate,\n",
    "            n_repeats=n_repeats, n_folds=n_folds)\n",
    "\n",
    "        print(f\"Done with {i}/10\")\n",
    "\n",
    "        results.update(new_result)\n",
    "\n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}